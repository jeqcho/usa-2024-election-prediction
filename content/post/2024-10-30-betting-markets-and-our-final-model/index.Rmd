---
title: Betting markets and our final model
author: Jay Chooi
date: '2024-10-30'
slug: betting-markets-and-our-final-model
categories: []
tags: []
---


Note: this is for the November 4 submission. See [here](/post/2024/10/26/putting-it-all-together-a-final-prediction-model/) for the 28 Oct submission.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
set.seed(123)

library(dplyr)
library(ranger)
```

## Betting odds

```{r}
bet <- read.csv("../data/betting_odds.csv") |>
  select(c("harris_odds","state"))

state_odds <- read.csv("../data/state_odds.csv")

bet$state <- abbr_to_full(bet$state)

bet$harris_odds <- as.numeric(sub("%", "", bet$harris_odds)) / 100

state_odds <- state_odds |>
  left_join(bet, by="state")
```

```{r}
state_odds <- state_odds |>
  rename(market_odds = "harris_odds", model_odds = "odds")
```

```{r}
state_odds$market_odds <- state_odds$market_odds * 100
```


```{r}
# Load necessary packages
library(ggplot2)
library(ggrepel)  # For better label placement

ggplot(state_odds, aes(x = model_odds, y = market_odds)) +
  geom_point(aes(color = model_odds < 50), size = 1) +  # Conditionally color points
  scale_color_manual(
    values = c("TRUE" = "red", "FALSE" = "blue"),        # Red if < 50, blue otherwise
    name = "Model Odds",                                 # Title for the legend
    labels = c("Harris >= 50%", "Harris < 50%")                          # Custom labels for the legend
  ) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Identity line
  geom_vline(xintercept = 50, linetype = "dotted") +  # Vertical line at 50 (not 0.5)
  geom_hline(yintercept = 50, linetype = "dotted") +  # Horizontal line at 50 (not 0.5)
  geom_text_repel(aes(label = state), size = 3) +  # Adding state labels with repelling
  labs(
    title = "Market Odds vs Model Odds for Harris",
    x = "Model Odds",
    y = "Market Odds"
  ) +
  theme_minimal()
```

```{r}
state_pv2p_pred <- data2024 |>
  select(c("state","preds"))

state_odds <- state_odds |>
  left_join(state_pv2p_pred)
```

```{r}
ggplot(state_odds, aes(x = preds, y = market_odds)) +
  geom_point(aes(color = model_odds < 50), size = 1) +  # Conditionally color points
  scale_color_manual(
    values = c("TRUE" = "red", "FALSE" = "blue"),        # Red if < 50, blue otherwise
    name = "Model Odds",                                 # Title for the legend
    labels = c("Harris >= 50%", "Harris < 50%")                          # Custom labels for the legend
  ) +
  geom_vline(xintercept = 50, linetype = "dotted") + # Vertical line at 0.5 (example)
  geom_hline(yintercept = 50, linetype = "dotted") + # Horizontal line at 0.5 (example)
  geom_text_repel(aes(label = state), size = 3) +  # Adding state labels with repelling
  labs(
    title = "Market Odds vs Predicted Two-Party Vote Share for Harris",
    x = "Predicted Two-Party Vote Share",
    y = "Market Odds"
  ) +
  theme_minimal()
```

```{r}
state_odds$preds_logit <- log(state_odds$preds/100/(1-state_odds$preds/100))

state_odds$market_odds_logit <- log(state_odds$market_odds/100/(1-state_odds$market_odds/100))
```






# Appendix

Here are other ideas that didn't improve the model.

## A later economy

Instead of Q2 GDP growth, Q3 GDP growth comparison is closer to the election.

```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q3_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q3_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```


## Partisanship

Voters might vote along partisan lines. Let's incorporate the party where the incumbent candidate is from.

```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~inc_poll2p + q2_gdp_growth + incumbent  + inc_lag+ deminc,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag + deminc,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```

## A longer horizon for the economy

How about using a GDP growth compared to 3 years ago?

```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag ,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~inc_poll2p + q2_gdp_growth_4y + incumbent  + inc_lag ,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```

## More sophisticated models

Let's see if using a random forest helps.

```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- ranger(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)$predictions) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- ranger(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag ,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)$predictions) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```

## Swing state behavior

Swing states might have inherently different dynamics than other states. For example, they are the battleground states where campaign spending are being focused on. Let's add a flag on whether a state is consider a swing state in that election.

```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag + swing,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag + swing,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```

## National GDP

Some literature suggests that using national-level GDP is more accurate than using state-level GDP.


```{r}
# Fit the model excluding observations with the same year
all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + national_q2_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
allrmse <- err
cat("State-level Abramowitz LOO-CV RMSE:", err, "\n")

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + national_q2_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr & state%in% swing_states)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
cat("State-level Abramowitz LOO-CV RMSE (swing-states):", err, "\n")
```


## Diagnostics

The diagnostics plots below show that the OLS assumptions hold pretty well (ELIH assumptions). The Q-Q plot in particular looks spectacular.

```{r}
model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag,
              data = state_popvote_gdp_polls)

plot(model)
```

```{r}
# Load the required library
library(ggplot2)

# Assuming your model is called `model` and training data is called `train_data`
# Calculate predicted values from the model
train_data <- state_popvote_gdp_polls
predicted_values <- predict(model, newdata = train_data)

# Calculate residuals (difference between actual values and predicted values)
residuals <- residuals(model)

# Assuming 'year' column is present in your 'train_data' but not used in the model
train_data$predicted <- predicted_values
train_data$residuals <- residuals

# Plot residuals vs predicted values and color-code by 'year'
ggplot(train_data, aes(x = predicted, y = residuals, color = factor(year))) +
  geom_point() +                          # Points for residuals vs predicted
  labs(x = "Predicted Values", y = "Residuals", title = "Residuals vs Predicted Values by Year") +
  theme_minimal() +                       # Use a minimal theme
  scale_color_discrete(name = "Year")     # Make sure the legend uses 'Year'
```

```{r}
model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag,
              data = state_popvote_gdp_polls)

# Calculate predicted values from the model
train_data <- state_popvote_gdp_polls  # Replace with your actual data

# Add predicted values to the original data
train_data$predicted <- predicted_values

all_preds <- data.frame(
  year=numeric(0),
  state=character(0),
  preds=numeric(0),
  inc_pv2p=numeric(0)
)
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  test_data$preds <- predict(model, test_data)
  
  result <- test_data |>
    select(c("year","state","preds","inc_pv2p"))
  
  # Collect all errors
  all_preds <- rbind(all_preds, result)
}

# Plot Fitted (Actual) vs Predicted
ggplot(all_preds, aes(x = inc_pv2p, y = preds)) +
  geom_point(aes(color = (inc_pv2p > 50) != (preds > 50)), size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Fitted vs LOO Predicted Incumbent Two-Party Popular Vote (2008-2020)",
    x = "Actual Incumbent Two-Party Vote Share",
    y = "LOO Predicted Incumbent Two-Party Vote Share"
  ) +
  scale_color_manual(
    values = c("TRUE" = "red", "FALSE" = "darkgreen"),
    labels = c("Correct Prediction", "Incorrect Prediction"),
    name = "Prediction Accuracy"
  ) +
  theme_minimal()
```

```{r}
# Subset data for critical predictions
crit_preds <- subset(all_preds, preds > 45 & preds < 55)

# Create a Prediction Accuracy variable
crit_preds$PredictionAccuracy <- ifelse(
  (crit_preds$inc_pv2p > 50) == (crit_preds$preds > 50),
  "Correct Prediction",
  "Incorrect Prediction"
)

# Plot
ggplot(crit_preds, aes(x = inc_pv2p, y = preds)) +
  # Highlight quadrants
  geom_rect(aes(xmin = 45, xmax = 50, ymin = 50, ymax = 55),
            fill = "lightpink", alpha = 0.3, inherit.aes = FALSE) +
  geom_rect(aes(xmin = 50, xmax = 55, ymin = 45, ymax = 50),
            fill = "lightblue", alpha = 0.3, inherit.aes = FALSE) +
  # Points colored by Prediction Accuracy
  geom_point(aes(color = PredictionAccuracy), size = 2) +
  # Diagonal reference line
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  # Labels
  labs(
    title = "Zoomed plot for predictions between 45% and 55% (2008-2020)",
    x = "Actual Incumbent Two-Party Vote Share",
    y = "LOO Predicted Incumbent Two-Party Vote Share"
  ) +
  # Custom color scale
  scale_color_manual(
    values = c("Correct Prediction" = "darkgreen", "Incorrect Prediction" = "red"),
    name = "Prediction Accuracy"
  ) +
  # Add quadrant labels
  annotate("text", x = 47.5, y = 54, label = "Actual Loss,\nPredicted Win", 
           color = "black", size = 3, hjust = 0.5) +
  annotate("text", x = 52.5, y = 46, label = "Actual Win,\nPredicted Loss", 
           color = "black", size = 3, hjust = 0.5) +
  # Add labels only for incorrect predictions
  geom_text_repel(
    data = subset(crit_preds, PredictionAccuracy == "Incorrect Prediction"),
    aes(label = paste(state, year)),
    size = 3
  ) +
  # Set plot limits and theme
  xlim(45, 55) +
  ylim(45, 55) +
  theme_minimal()
```



