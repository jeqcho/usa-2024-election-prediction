---
title: Incumbency Helps; Experts are Right
author: Jay Chooi
date: '2024-09-25'
slug: incumbency
categories: []
tags: []
---

# Incumbency helps

Take a look at this plot with data from 1948 to 2020.

```{r include=FALSE}
popvote <- read.csv("popvote_1948-2020.csv")
```

```{r echo=FALSE}
# Load necessary libraries
library(ggplot2)

# Step 1: Separate incumbents and non-incumbents
incumbent_data <- subset(popvote, !is.na(incumbent) &
                           !is.na(winner) & incumbent == TRUE)
non_incumbent_data <- subset(popvote, !is.na(incumbent) &
                               !is.na(winner)  & incumbent == FALSE)

# Step 2: Calculate win percentages for incumbents and non-incumbents
incumbent_win_count <- sum(incumbent_data$winner == TRUE)
incumbent_total <- nrow(incumbent_data)
incumbent_win_percentage <- (incumbent_win_count / incumbent_total) * 100

non_incumbent_win_count <- sum(non_incumbent_data$winner == TRUE)
non_incumbent_total <- nrow(non_incumbent_data)
non_incumbent_win_percentage <- (non_incumbent_win_count / non_incumbent_total) * 100

# Step 3: Create a data frame for plotting
win_percentage_data <- data.frame(
  Group = c("Incumbent", "Non-Incumbent"),
  WinPercentage = c(incumbent_win_percentage, non_incumbent_win_percentage)
)

# Step 4: Plot the win percentages
ggplot(win_percentage_data, aes(x = Group, y = WinPercentage, fill = Group)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(aes(label = sprintf("%.1f%%", WinPercentage)), 
            vjust = -0.5, size = 5) +  
  scale_fill_manual(values = c("Incumbent" = "chartreuse3", "Non-Incumbent" = "orange")) + 
  ggtitle("Win Percentage: Incumbents vs Non-Incumbents") +
  ylab("Win Percentage (%)") +
  xlab("Group") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 100))
```
The plot tells us that an incumbent president has an edge over their competitor. This effect is known as the incumbent advantage, and is well-studied in academia. (The incumbent advantage shows up everywhere around the world, curiously except in [South America](https://www.jstor.org/stable/26288984)).

One might argue that there is no incumbent-candidate effect, but merely the incumbent-party effect. Let's test this theory by looking at the incumbent vs non-incumbent win rate by conditioning on being in a incumbent party

```{r echo=FALSE}
# Step 1: Separate incumbents and non-incumbents
incumbent_party_incumbent_data <- subset(incumbent_data, incumbent_party == TRUE)
incumbent_party_non_incumbent_data <- subset(non_incumbent_data, incumbent_party == TRUE)

# Step 2: Calculate win percentages for incumbents and non-incumbents
incumbent_win_count <- sum(incumbent_party_incumbent_data$winner == TRUE)
incumbent_total <- nrow(incumbent_party_incumbent_data)
incumbent_win_percentage <- (incumbent_win_count / incumbent_total) * 100

non_incumbent_win_count <- sum(incumbent_party_non_incumbent_data$winner == TRUE)
non_incumbent_total <- nrow(incumbent_party_non_incumbent_data)
non_incumbent_win_percentage <- (non_incumbent_win_count / non_incumbent_total) * 100

# Step 3: Create a data frame for plotting
win_percentage_data <- data.frame(
  Group = c("Incumbent", "Non-Incumbent"),
  WinPercentage = c(incumbent_win_percentage, non_incumbent_win_percentage)
)

# Step 4: Plot the win percentages
ggplot(win_percentage_data, aes(x = Group, y = WinPercentage, fill = Group)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(aes(label = sprintf("%.1f%%", WinPercentage)), 
            vjust = -0.5, size = 5) +  
  scale_fill_manual(values = c("Incumbent" = "chartreuse3", "Non-Incumbent" = "orange")) + 
  ggtitle("Win Percentage: Incumbents vs Non-incumbents of Incumbent Party") +
  ylab("Win Percentage (%)") +
  xlab("Group") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 100))
```
It turns out that non-incumbents in a incumbent party fared worse than the overall non-incumbents! Let's take a closer look
```{r echo=FALSE}
# Step 1: Separate incumbents and non-incumbents
non_incumbent_party_non_incumbent_data <- subset(non_incumbent_data, incumbent_party == FALSE)
incumbent_party_non_incumbent_data <- subset(non_incumbent_data, incumbent_party == TRUE)

# Step 2: Calculate win percentages for incumbents and non-incumbents
incumbent_win_count <- sum(non_incumbent_party_non_incumbent_data$winner == TRUE)
incumbent_total <- nrow(non_incumbent_party_non_incumbent_data)
incumbent_win_percentage <- (incumbent_win_count / incumbent_total) * 100

non_incumbent_win_count <- sum(incumbent_party_non_incumbent_data$winner == TRUE)
non_incumbent_total <- nrow(incumbent_party_non_incumbent_data)
non_incumbent_win_percentage <- (non_incumbent_win_count / non_incumbent_total) * 100

# Step 3: Create a data frame for plotting
win_percentage_data <- data.frame(
  Group = c("Non-Incumbent Party", "Incumbent Party"),
  WinPercentage = c(incumbent_win_percentage, non_incumbent_win_percentage)
)

# Step 4: Plot the win percentages
ggplot(win_percentage_data, aes(x = Group, y = WinPercentage, fill = Group)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(aes(label = sprintf("%.1f%%", WinPercentage)), 
            vjust = -0.5, size = 5) +  
  scale_fill_manual(values = c("Non-Incumbent Party" = "firebrick1", "Incumbent Party" = "dodgerblue4")) + 
  ggtitle("Win Percentage of Non-Incumbents from Different Parties") +
  ylab("Win Percentage (%)") +
  xlab("Group") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 100))
```


These two plots tell us two things
(i) There is an overall incumbent candidate effect: incumbent candidates (66.7%) performed better than non-incumbents (42.3%).
(ii) There is a *negative* incumbent party effect: non-incumbents in an incumbent party (14.3%) performed worse than non-incumbents in a non-incumbent party (52.6%)

Trump is a non-incumbent from a non-incumbent party, while Harris is a non-incumbent from an incumbent-party (hence the color coding above), which means that the effect is acting against Harris.

Let's figure out what's causing the incumbency advantage. One hypothesis is that incumbents will award more federal spending grants to key states which will sway votes to their side (academics call this pork-barrel spending). Here we test that effect by plotting the change in vote share vs change in federal spending for counties from 1996 to 2008.


```{r include=FALSE}
library(dplyr)
fed.grants.county <- read.csv("fedgrants_bycounty_1988-2008.csv") |>
  filter(year>=1992) |>
  filter(!is.na(dvoteswing_inc)&!is.na(dpct_grants)) |>
  filter(!is.na(dpc_income))
```


```{r echo=FALSE,results="asis", warning=FALSE, message=FALSE}
# install.packages("kableExtra")
# install.packages("broom")
library(broom)
library(kableExtra)
# Step 1: Fit the linear model
model <- lm(dvoteswing_inc ~ dpct_grants, data = fed.grants.county)
library(xtable)

# Step 2: Display a summary of the model to see the results
print(xtable(summary(model)),type='html')
```
```{r}
summary(model)$adj.r.squared
```



```{r echo=FALSE, message=FALSE}
# Step 3: Create the scatterplot with regression line
ggplot(fed.grants.county, aes(x = dpct_grants, y = dvoteswing_inc)) +
  geom_point(alpha=0.2) +   # Plot points with a specific color and size
  geom_smooth(method = "lm", col = "red", se = TRUE) +  # Add a regression line with confidence interval
  ggtitle("Scatterplot of Vote Swing for Incumbents vs Federal Aid Change") +
  xlab("Percentage Change in Federal Aid") +
  ylab("Percentage Change in Incumbent Vote Share") +
  theme_minimal()
```
The low \(R^2\) of 0.002 shows that at best, federal spending only explains 0.2% of the vote share. But note the low p-value that is less than \(10^{-10}\), with an estimated effect of -0.007. This means that the model is confident that federal spending *decreases* vote share by 0.007% for every 1% increase in federal spending! This is the opposite of what we expected. Instead of what the *government* does (like federal spending), let's see how the *people* feel by fitting vote share change against county-level income change

```{r echo=FALSE, results="asis"}
# Step 1: Fit the linear model
model <- lm(dvoteswing_inc ~ dpc_income, data = fed.grants.county)

# Step 2: Display a summary of the model to see the results
library(xtable)

# Step 2: Display a summary of the model to see the results
print(xtable(summary(model)),type='html')
```

```{r}
summary(model)$adj.r.squared
```

```{r message=FALSE, echo=FALSE}
# Step 3: Create the scatterplot with regression line
ggplot(fed.grants.county, aes(x = dpc_income, y = dvoteswing_inc)) +
  geom_point(alpha=0.2) +   # Plot points with a specific color and size
  geom_smooth(method = "lm", col = "red", se = TRUE) +  # Add a regression line with confidence interval
  ggtitle("Scatterplot of Vote Swing for Incumbents vs Income Change") +
  xlab("Percentage Change in Income") +
  ylab("Percentage Change in Incumbent Vote Share") +
  theme_minimal()
```
The \(R^2\) is lower, explaining away 0.02% of the variance, but the p-value is at 0.04 with an estimate of 0.06. This means that a 1% increase in income is associated with a 0.06% increase in vote share for the incumbent party. That means that if many counties face an increase in income over the term, the incumbent president will be favored. And indeed this is the case:

```{r echo=FALSE}
# Step 1: Categorize dpc_income into bins
fed.grants.county$dpc_income_bins <- cut(
  fed.grants.county$dpc_income, 
  breaks = c(-Inf, -5, -1, 0,1,  5, Inf), 
  labels = c("More than -5%", "-5% to -1%", "-1% to 0%", "0% to 1%", "1% to 5%", "More than 5%"),
  right = FALSE  # Include lower limit in the bin
)

# Step 2: Create a histogram of the binned dpc_income data
ggplot(fed.grants.county|>filter(!is.na(dpc_income_bins)), aes(x = dpc_income_bins)) +
  geom_bar(fill = "skyblue", color = "black") +  # Bar plot since it's binned data
  ggtitle("Histogram of Percentage Change in Per Capita Income") +
  xlab("Percentage Change in Per Capita Income") +
  ylab("Frequency") +
  theme_minimal()
```
The plot above shows that thanks to the current trend of the modern economy, most counties face an increase in income. It can then be theorized that the incumbency advantage is explained by the momentum of the modern economy making the constituents to reward the incumbent president, whoever they are.

This might be disappointing to some readers, as they would want presidents who *tried harder* by allocating more federal spending on key states to be favored, but in reality the mechanism of democracy will ensure that those favored are presidents who made the situation on the ground *actually better* regardless of their federal spending. And because of the modern economy momentum, this is anyone who manages to get into the Oval Office.

# Simple models can be powerful

Here's the Abramowitz's Time for Change model.

\[
\underbrace{\text{pv2p}}_{\text{incumbent party}} = \beta_0 + \beta_1 \cdot \underbrace{\text{G2GDP}}_{\text{Q2 GDP growth}} + \beta_2 \cdot \underbrace{\text{NETAPP}}_{\text{June Gallup job approval}} + \beta_3 \cdot \underbrace{\text{TERM1INC}}_{\text{sitting pres}}
\]

It is a simple linear regression using three variables. Let's pit this against a random forest, which is a state-of-the-art statistical technique for tabular data. We will use the same three variables: Q2 GDP growth, June Gallup job approval and the incumbency indicator to predict the incumbent party vote share. Let's test it using leave-one-out cross validation from 1948 to 2020.

```{r include=FALSE}
library(dplyr)

fred_econ <- read.csv("fred_econ.csv")
fred_econ <- fred_econ |>
  filter(quarter==2, !is.na(GDP_growth_quarterly)) |>
  select(year, GDP_growth_quarterly) |>
  rename(q2_gdp_growth = GDP_growth_quarterly)

popvote <- popvote |>
  left_join(fred_econ)
```

```{r include=FALSE}
popvote_incumbent_party <- popvote |>
  filter(incumbent_party == TRUE, !is.na(pv2p))
```

```{r echo=FALSE, message=FALSE}
# Load necessary libraries
library(randomForest)
library(caret)  # For leave-one-out cross-validation

# Random Forest model (set a seed for reproducibility)
set.seed(123)
rf_model <- randomForest(pv2p ~ juneapp + q2_gdp_growth + incumbent, data = popvote_incumbent_party)

# Step 1: Perform Leave-One-Out Cross-Validation for the linear model
lm_loo_errors <- sapply(1:nrow(popvote_incumbent_party), function(i) {
  # Fit the model excluding observation i
  lm_loo <- lm(pv2p ~ juneapp + q2_gdp_growth + incumbent, data = popvote_incumbent_party[-i, ])
  # Predict the excluded observation
  prediction <- predict(lm_loo, newdata = popvote_incumbent_party[i, ])
  # Calculate the squared error for the excluded observation
  (popvote_incumbent_party$pv2p[i] - prediction)^2
})

# Step 2: Perform Leave-One-Out Cross-Validation for the random forest model
rf_loo_errors <- sapply(1:nrow(popvote_incumbent_party), function(i) {
  # Fit the model excluding observation i
  rf_loo <- randomForest(pv2p ~ juneapp + q2_gdp_growth + incumbent, data = popvote_incumbent_party[-i, ])
  # Predict the excluded observation
  prediction <- predict(rf_loo, newdata = popvote_incumbent_party[i, ])
  # Calculate the squared error for the excluded observation
  (popvote_incumbent_party$pv2p[i] - prediction)^2
})

# Step 3: Calculate the Root Mean Squared Error (RMSE) for both models
lm_rmse <- sqrt(mean(lm_loo_errors))  # Take the square root of the mean of the squared errors
rf_rmse <- sqrt(mean(rf_loo_errors))  # Same for random forest

# Step 4: Print the results
cat("Linear Model LOO-CV RMSE:", lm_rmse, "\n")
cat("Random Forest LOO-CV RMSE:", rf_rmse, "\n")
```

The RMSE for random forest is slightly lower than that of the Time for Change model. Let's take a closer look and see how they perform for each year.

```{r echo=FALSE, message=FALSE}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(randomForest)
library(tidyr)  # Load the tidyr package for pivot_longer

# Initialize vectors to store LOO predictions
lm_loo_predictions <- numeric(nrow(popvote_incumbent_party))
rf_loo_predictions <- numeric(nrow(popvote_incumbent_party))

# Step 1: Perform Leave-One-Out Cross-Validation (LOO-CV) for both models
for (i in 1:nrow(popvote_incumbent_party)) {
  
  # Leave out the ith observation for LOO-CV
  train_data <- popvote_incumbent_party[-i, ]
  test_data <- popvote_incumbent_party[i, , drop = FALSE]
  
  # Fit linear model on the training data
  lm_loo_model <- lm(pv2p ~ juneapp + q2_gdp_growth + incumbent, data = train_data)
  
  # Predict on the left-out observation
  lm_loo_predictions[i] <- predict(lm_loo_model, newdata = test_data)
  
  # Fit random forest model on the training data
  rf_loo_model <- randomForest(pv2p ~ juneapp + q2_gdp_growth + incumbent, data = train_data)
  
  # Predict on the left-out observation
  rf_loo_predictions[i] <- predict(rf_loo_model, newdata = test_data)
}

# Step 2: Calculate the deviations (lm and rf predictions from the truth)
prediction_data <- popvote_incumbent_party %>%
  mutate(lm_loo_predictions = lm_loo_predictions,
         rf_loo_predictions = rf_loo_predictions) %>%
  select(year, pv2p, lm_loo_predictions, rf_loo_predictions) %>%
  rename(truth = pv2p) %>%
  mutate(lm_deviation = lm_loo_predictions - truth,  # Deviation of lm predictions from truth
         rf_deviation = rf_loo_predictions - truth)  # Deviation of rf predictions from truth

# Step 3: Reshape the data for plotting (convert to long format)
plot_data <- prediction_data %>%
  select(year, lm_deviation, rf_deviation) %>%
  pivot_longer(cols = c(lm_deviation, rf_deviation), 
               names_to = "model", 
               values_to = "deviation")

plot_data$model <- ifelse(plot_data$model=="lm_deviation","Time for Change", "Random forest")

# Step 4: Plot the bar chart showing deviations from truth
ggplot(plot_data, aes(x = factor(year), y = deviation, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Bar chart with dodged bars
  ggtitle("Deviation of LM and RF Predictions from Truth by Year") +
  xlab("Year") +
  ylab("Deviation from Truth (pv2p)") +
  scale_fill_manual(values = c("Time for Change" = "chartreuse3", "Random forest" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```
Now it appears that 2020 is a huge outlier for the Time for Change model. Since 2020 is a special year with the COVID-19 pandemic, Abramowitz revised his model to use the following instead.

\[
\underbrace{\text{incumbent party}}_\text{pv2p} = \beta_0 + \beta_1 \cdot \underbrace{\text{June Gallup job approval}}_\text{NETAPP}
\]

Now let's see how does this compare with the random forest, using only one variable.

```{r echo=FALSE}
# Load necessary libraries
library(randomForest)
library(caret)  # For leave-one-out cross-validation

# Random Forest model (set a seed for reproducibility)
set.seed(123)

# Step 1: Perform Leave-One-Out Cross-Validation for the linear model
lm_loo_errors <- sapply(1:nrow(popvote_incumbent_party), function(i) {
  # Fit the model excluding observation i
  lm_loo <- lm(pv2p ~ juneapp , data = popvote_incumbent_party[-i, ])
  # Predict the excluded observation
  prediction <- predict(lm_loo, newdata = popvote_incumbent_party[i, ])
  # Calculate the squared error for the excluded observation
  (popvote_incumbent_party$pv2p[i] - prediction)^2
})

# Step 2: Perform Leave-One-Out Cross-Validation for the random forest model
rf_loo_errors <- sapply(1:nrow(popvote_incumbent_party), function(i) {
  # Fit the model excluding observation i
  rf_loo <- randomForest(pv2p ~ juneapp , data = popvote_incumbent_party[-i, ])
  # Predict the excluded observation
  prediction <- predict(rf_loo, newdata = popvote_incumbent_party[i, ])
  # Calculate the squared error for the excluded observation
  (popvote_incumbent_party$pv2p[i] - prediction)^2
})

# Step 3: Calculate the Root Mean Squared Error (RMSE) for both models
lm_rmse <- sqrt(mean(lm_loo_errors))  # Take the square root of the mean of the squared errors
rf_rmse <- sqrt(mean(rf_loo_errors))  # Same for random forest

# Step 4: Print the results
cat("Linear Model Lite LOO-CV RMSE:", lm_rmse, "\n")
cat("Random Forest Lite LOO-CV RMSE:", rf_rmse, "\n")
```

Incredibly, this one-variable linear model has a lower RMSE than our random forest. Perhaps even more surprisingly, this linear model even *outperformed* our two models above! It recorded a RMSE of 3.23 against the other models having RMSE 4.183061, 3.59677 and 3.799247.


```{r echo=FALSE}
# Initialize vectors to store LOO predictions
lm_loo_predictions <- numeric(nrow(popvote_incumbent_party))
rf_loo_predictions <- numeric(nrow(popvote_incumbent_party))

# Step 1: Perform Leave-One-Out Cross-Validation (LOO-CV) for both models
for (i in 1:nrow(popvote_incumbent_party)) {
  
  # Leave out the ith observation for LOO-CV
  train_data <- popvote_incumbent_party[-i, ]
  test_data <- popvote_incumbent_party[i, , drop = FALSE]
  
  # Fit linear model on the training data
  lm_loo_model <- lm(pv2p ~ juneapp, data = train_data)
  
  # Predict on the left-out observation
  lm_loo_predictions[i] <- predict(lm_loo_model, newdata = test_data)
  
  # Fit random forest model on the training data
  rf_loo_model <- randomForest(pv2p ~ juneapp, data = train_data)
  
  # Predict on the left-out observation
  rf_loo_predictions[i] <- predict(rf_loo_model, newdata = test_data)
}

# Step 2: Calculate the deviations (lm and rf predictions from the truth)
prediction_data <- popvote_incumbent_party %>%
  mutate(lm_loo_predictions = lm_loo_predictions,
         rf_loo_predictions = rf_loo_predictions) %>%
  select(year, pv2p, lm_loo_predictions, rf_loo_predictions) %>%
  rename(truth = pv2p) %>%
  mutate(lm_deviation = lm_loo_predictions - truth,  # Deviation of lm predictions from truth
         rf_deviation = rf_loo_predictions - truth)  # Deviation of rf predictions from truth

# Step 3: Reshape the data for plotting (convert to long format)
plot_data <- prediction_data %>%
  select(year, lm_deviation, rf_deviation) %>%
  pivot_longer(cols = c(lm_deviation, rf_deviation), 
               names_to = "model", 
               values_to = "deviation")

plot_data$model <- ifelse(plot_data$model=="lm_deviation","Time for Change", "Random forest")

# Step 4: Plot the bar chart showing deviations from truth
ggplot(plot_data, aes(x = factor(year), y = deviation, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Bar chart with dodged bars
  ggtitle("Deviation of LM and RF Predictions from Truth by Year") +
  xlab("Year") +
  ylab("Deviation from Truth (pv2p)") +
  scale_fill_manual(values = c("Time for Change" = "chartreuse3", "Random forest" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```
Indeed, it appears that the simple model is closer to the truth than the complicated random forest model for most years. One might be tempted to ask the forecasters to pack their bags and crown the linear model with approval ratings as the best model, but let's take a closer look at the $R^2$ for the whole dataset.

```{r}
summary(lm(pv2p ~ juneapp, popvote_incumbent_party))$adj.r.squared
```

The simple model can explain away 64.9% of the variance, but there is still work to be done on the 35.1%, where we can use this simple model as part of an ensemble model to reduce the variance. Moving on, let's look at other expert predictions, Sabato and Cook, from their performance on their predictions.

# Expert predictions are accurate

Here are the historical performances of [Sabato](https://centerforpolitics.org/crystalball/) and [Cook](https://www.cookpolitical.com/) from 2004 to 2020.

```{r include=FALSE}
sabato <- read.csv("sabato_crystal_ball_ratings.csv") |>
  rename(abbr=state)
cook <- read.csv("CPR_EC_Ratings.csv") |>
  rename(abbr=Abbreviation, year=Cycle, rating=Rating)
```

```{r include=FALSE}
# Perform an inner join
experts <- sabato %>%
  inner_join(cook, by = c("year","abbr")) |>
  rename(sabato_rating=rating.x, cook_rating=rating.y)

# Create a mapping for cook_rating
cook_rating_mapping <- c(
  "Solid D"  = 1,  # Solid Democrat
  "Likely D" = 2,  # Likely Democrat
  "Lean D"   = 3,  # Lean Democrat
  "Toss Up"  = 4,  # Toss Up
  "Lean R"   = 5,  # Lean Republican
  "Likely R" = 6,  # Likely Republican
  "Solid R"  = 7   # Solid Republican
)

# Apply the mapping to experts$cook_rating
experts$cook_rating <- cook_rating_mapping[experts$cook_rating]
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Step 1: Check if the prediction was correct based on PluralityParty (winning party)
experts <- experts %>%
  mutate(
    sabato_correct = (sabato_rating <= 3 & PluralityParty == "D") |  # D leaning and D won
                     (sabato_rating >= 5 & PluralityParty == "R"),   # R leaning and R won
    cook_correct = (cook_rating <= 3 & PluralityParty == "D") |      # D leaning and D won
                   (cook_rating >= 5 & PluralityParty == "R")        # R leaning and R won
  ) |> filter(year>2000)

# Step 2: Calculate the total number of predictions within each group for Sabato and Cook ratings
total_predictions_sabato <- experts %>%
  group_by(sabato_rating) %>%
  summarise(total = n())

total_predictions_cook <- experts %>%
  group_by(cook_rating) %>%
  summarise(total = n())

# Step 3: Calculate the correct predictions and percentages within each rating group

# For Sabato ratings
correct_predictions_sabato <- experts %>%
  filter(sabato_correct == TRUE) %>%
  group_by(sabato_rating) %>%
  summarise(correct_count = n()) %>%
  left_join(total_predictions_sabato, by = "sabato_rating") %>%
  mutate(
    percentage = (correct_count / total) * 100,  # Calculate percentage within group
    rating_system = "Sabato"  # Add a column for rating system
  )

# For Cook ratings
correct_predictions_cook <- experts %>%
  filter(cook_correct == TRUE) %>%
  group_by(cook_rating) %>%
  summarise(correct_count = n()) %>%
  left_join(total_predictions_cook, by = "cook_rating") %>%
  mutate(
    percentage = (correct_count / total) * 100,  # Calculate percentage within group
    rating_system = "Cook"  # Add a column for rating system
  )

# Step 4: Combine the data for both Sabato and Cook ratings
combined_data <- bind_rows(
  correct_predictions_sabato %>% rename(score = sabato_rating),
  correct_predictions_cook %>% rename(score = cook_rating)
)

# Step 5: Convert the score column to a factor with levels from 1 to 7 to ensure all ratings are shown
combined_data <- combined_data %>%
  mutate(score = factor(score, levels = 1:7))  # Explicitly set levels 1 to 7

# Step 6: Create the grouped bar chart with percentages and labels on the bars
ggplot(combined_data, aes(x = score, y = percentage, fill = rating_system)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.1f", percentage)), 
            position = position_dodge(width = 0.9), vjust = -0.5, size = 2) +  # Add percentage labels
  ggtitle("Correctness Within Each Group for Sabato and Cook Ratings") +
  xlab("Rating") +
  ylab("Percentage of Correct Predictions (%)") +
  labs(fill="Expert") +
  scale_fill_manual(values = c("Sabato" = "chocolate2", "Cook" = "navy")) +
  scale_x_discrete(labels = c("1" = "Solid D", "2" = "Likely D", "3" = "Lean D", 
                              "4" = "Toss Up", "5" = "Lean R", "6" = "Likely R", 
                              "7" = "Solid R")) +  # Set custom x-axis labels
  theme_minimal()
```

Cook predictions are spot on with 100% accuracy, except for Lean D groups where Democrats only won 89.7% of the time. Sabato's accuracies are lower, with 78.6% for Lean D, and also 95.2% for Likely D, 83.3% for Lean R and 98.9% for Solid R. The nice dip in the middle shows that the confidence reported by these sources are roughly correct. Now the question begs: why is Cook more accurate than Sabato? In our plot above, we can't evaluate their performances when a state is categorized as a Toss-Up. Here's a plot of the number of toss-ups categorized by each source over time.

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(scales)

# Step 1: Filter the data to include only "Toss Up" predictions for Sabato
toss_up_data_sabato <- sabato %>%
  filter(rating == 4) %>%  # Include Toss Up ratings (assumed to be 4)
  group_by(year) %>%
  summarise(toss_up_count = n())  # Count the number of Toss Up predictions each year

# Ensure all years are included in Sabato data
all_years_sabato <- sabato %>%
  distinct(year)

# Merge toss-up data for Sabato with all years, filling missing values with 0
toss_up_data_sabato_complete <- all_years_sabato %>%
  left_join(toss_up_data_sabato, by = "year") %>%
  mutate(toss_up_count = ifelse(is.na(toss_up_count), 0, toss_up_count))

# Step 2: Filter the data to include only "Toss Up" predictions for Cook
toss_up_data_cook <- cook %>%
  filter(rating == "Toss Up") %>%
  group_by(year) %>%
  summarise(toss_up_count = n())

# Ensure all years are included in Cook data
all_years_cook <- cook %>%
  distinct(year)

# Merge toss-up data for Cook with all years, filling missing values with 0
toss_up_data_cook_complete <- all_years_cook %>%
  left_join(toss_up_data_cook, by = "year") %>%
  mutate(toss_up_count = ifelse(is.na(toss_up_count), 0, toss_up_count))

# Step 3: Manually add the 2024 data point for Cook with a toss-up count of 7
toss_up_data_cook_complete <- toss_up_data_cook_complete %>%
  bind_rows(data.frame(year = 2024, toss_up_count = 7))

# Step 4: Combine the two datasets for plotting
toss_up_data_combined <- toss_up_data_sabato_complete %>%
  mutate(source = "Sabato") %>%
  bind_rows(toss_up_data_cook_complete %>%
              mutate(source = "Cook"))

# Step 5: Create the line plot to show the number of Toss Ups over time for both sources
ggplot(toss_up_data_combined, aes(x = year, y = toss_up_count, color = source, group = source)) +
  geom_line(size = 1) +  # Create line plot for both Sabato and Cook
  geom_point(size = 2) +  # Add points for each year for both sources
  ggtitle("Number of Toss Ups Over Time (Sabato vs. Cook)") +
  xlab("Year") +
  ylab("Number of Toss Up Predictions") +
  scale_x_continuous(breaks = toss_up_data_combined$year) +  # Show all years on the x-axis
  scale_y_continuous(breaks = pretty_breaks(n = 10), labels = label_number(accuracy = 1)) +  # Ensure integer ticks on y-axis
  theme_minimal() +
  scale_color_manual(values = c("navy", "chocolate2"))  # Assign different colors to Sabato and Cook
```

It turns out that Cook has more toss-ups over the years than Sabato. In other words, Cook is playing it safe while Sabato is calling more bets, which can explain some part of why Sabato is less accurate: because Sabato is taking on more risks.

In the history of Sabato predictions, 2024 has the highest number of toss-ups (7) compared to the past three elections, until 2008 with Obama's election having a higher toss-up count of 9 states. For the first time, Cook also agrees with Sabato with the same 7 toss-up states in 2024 (this might suggest that other states are facing political calcification). But with 7 states on the fence, this means that this election is the most uncertain election in the past 12 years.





