---
title: Post-election Reflection
author: Jay Chooi
date: '2024-11-09'
slug: post-election-reflection
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
set.seed(123)

library(dplyr)
library(ranger)
library(ggplot2)
library(ggrepel)
library(tidyverse)
```

```{r}
real_swing_states <- c(
  "Arizona",
  "Nevada",
  "Georgia",
  "North Carolina",
  "Pennsylvania",
  "Michigan",
  "Wisconsin"
)

competitive_states <- c("Florida", "North Carolina", "Ohio", "Virginia",
                  "Arizona", "Maine", "Michigan", "Minnesota",
                  "Nevada", "New Hampshire", "Pennsylvania",
                  "Wisconsin", "Georgia", "Iowa")

swing_states <- real_swing_states
```


# A recap of our model

Our model is a simple OLS with 4 predictors and an intercept.

\[
\text{pv2p} = \beta_0 + \beta_1 \cdot \text{G2GDP} + \beta_2 \cdot \text{NETAPP} + \beta_3 \cdot \text{TERM1INC} + \beta_4 \cdot \text{LAG}
\]

Where:

| Variable        | Explanation                               |
|-----------------|-------------------------------------------|
| \(\text{pv2p}\)     | State-level incumbent party pv2p        |
| \(\text{G2GDP}\)    | State-level Q2 GDP growth               |
| \(\text{NETAPP}\)   | State-level two-candidate poll          |
| \(\text{TERM1INC}\) | Sitting president?                      |
| \(\text{LAG}\)      | State-level previous election pv2p      |


```{r}
# Function to map state abbreviations to full names
abbr_to_full <- function(state_abbr) {
  # Create a named vector where the names are abbreviations and the values are full names
  state_map <- c(
    "AL" = "Alabama", "AK" = "Alaska", "AZ" = "Arizona", "AR" = "Arkansas", "CA" = "California",
    "CO" = "Colorado", "CT" = "Connecticut", "DE" = "Delaware", "FL" = "Florida", "GA" = "Georgia",
    "HI" = "Hawaii", "ID" = "Idaho", "IL" = "Illinois", "IN" = "Indiana", "IA" = "Iowa",
    "KS" = "Kansas", "KY" = "Kentucky", "LA" = "Louisiana", "ME" = "Maine", "MD" = "Maryland",
    "MA" = "Massachusetts", "MI" = "Michigan", "MN" = "Minnesota", "MS" = "Mississippi", "MO" = "Missouri",
    "MT" = "Montana", "NE" = "Nebraska", "NV" = "Nevada", "NH" = "New Hampshire", "NJ" = "New Jersey",
    "NM" = "New Mexico", "NY" = "New York", "NC" = "North Carolina", "ND" = "North Dakota", "OH" = "Ohio",
    "OK" = "Oklahoma", "OR" = "Oregon", "PA" = "Pennsylvania", "RI" = "Rhode Island", "SC" = "South Carolina",
    "SD" = "South Dakota", "TN" = "Tennessee", "TX" = "Texas", "UT" = "Utah", "VT" = "Vermont",
    "VA" = "Virginia", "WA" = "Washington", "WV" = "West Virginia", "WI" = "Wisconsin", "WY" = "Wyoming", "DC" = "District Of Columbia"
  )
  
  # Map the state abbreviations to their full names
  return(state_map[state_abbr])
}
```



```{r}
# state results from class
# state_results <- read.csv("../data/state_votes_pres_2024.csv")
# state_results <- state_results[state_results$FIPS != "fips",]
# state_results <- state_results |>
#   rename(state=Geographic.Name,
#          total_votes=Total.Vote,
#          harris=Kamala.D..Harris,
#          trump=Donald.J..Trump
#          )

# state_results$harris <- as.numeric(state_results$harris)
# state_results$trump <- as.numeric(state_results$trump)
# state_results$inc_pv2p <- state_results$harris / (state_results$harris + state_results$trump) * 100
# truth2024 <- state_results |>
#   select(state,inc_pv2p)
```

```{r}
# another way to get the state results (by scraping)
truth2024 <- read.csv("../data/results.csv")
truth2024 <- truth2024 |>
  mutate(inc_pv2p = harris_votes/(trump_votes+harris_votes)*100) |>
  select(state,inc_pv2p)
```

```{r}
# combine all data including 2024
# and rename them
data2024 <- read.csv("../data/data2024.csv")
data2024$year <- 2024
data2024$X <- NULL
data2024$preds <- NULL
data2024 <- data2024 |>
  left_join(truth2024,by="state")

state_popvote_gdp_polls <- read.csv("../data/state_popvote_gdp_polls.csv")

all_data <- state_popvote_gdp_polls |>
  select(colnames(data2024))

all_data <- rbind(all_data, data2024)
```

```{r}

all_errors <- c()
YEARS <- unique(state_popvote_gdp_polls$year)

for (pred_yr in YEARS) {
  # Subset training data excluding the prediction year
  train_data <- subset(state_popvote_gdp_polls, year != pred_yr)
  
  # Fit the model using training data
  model <- lm(formula = inc_pv2p ~ inc_poll2p + q2_gdp_growth + incumbent  + inc_lag,
              data = train_data)
  
  # Subset test data for the prediction year and swing states
  test_data <- subset(state_popvote_gdp_polls,
                      year == pred_yr)
  
  # Compute squared errors for the test data
  cur_error <- (test_data$inc_pv2p - predict(model, test_data)) ^ 2
  
  # Collect all errors
  all_errors <- c(all_errors, cur_error)
}

# Compute and return RMSE
err <- round(sqrt(mean(all_errors)),2)
allrmse <- err

```



```{r}


all_data <- all_data %>%
  rename(
    pv2p = inc_pv2p,
    Q2GDP = q2_gdp_growth,
    POLL = inc_poll2p,
    SIT = incumbent,
    LAG = inc_lag
  )
```

```{r}
# add the model predictions
for (yr in unique(all_data$year)) {
  # Split data into training and test sets
  train_data <- all_data |> filter(year != yr)
  test_data <- all_data |> filter(year == yr)
  
  # Fit models on training data
  nopoll_model <- lm(formula = pv2p ~ Q2GDP + SIT + LAG, data = train_data)
  poll_model <- lm(formula = pv2p ~ Q2GDP + POLL + SIT + LAG, data = train_data)
  
  # Generate predictions for test data
  add <- data.frame(
    preds = predict(poll_model, test_data),
    nopoll_preds = predict(nopoll_model, test_data)
  )
  
  # Combine predictions with the test data
  test_data <- test_data |> mutate(preds = add$preds, nopoll_preds = add$nopoll_preds)
  
  # Update all_data with the test_data containing predictions
  all_data[all_data$year == yr, c("preds", "nopoll_preds")] <- test_data[, c("preds", "nopoll_preds")]
}
```

```{r}
# save this data
write.csv(all_data, "../data/all_data.csv")
```

We predicted a Trump win with 312 against 226 electoral votes. Here are the actual results `pv2p` and the prediction `preds`.

```{r}
all_data|>filter(year==2024)|>select(-c(year,nopoll_preds)) |>
  mutate(POLL=round(POLL,1),LAG=round(LAG,1),pv2p=round(pv2p,1),preds=round(preds,1))
```


```{r}
plotdata2024 <- all_data|>filter(year==2024)
plotdata2024$state <- factor(plotdata2024$state, levels = plotdata2024$state)



ggplot(plotdata2024, aes(x = state, y = preds)) +
  geom_point(aes(color = preds < 50), size = 1, shape = 16) +  # Dots for predicted vote share
  geom_point(aes(y = pv2p, shape = "Observed", color = pv2p < 50), size = 2) +  # New marker for observed vote share
  geom_errorbar(aes(ymin = preds - allrmse, ymax = preds + allrmse), width = 0.2) +  # Error bars
  scale_color_manual(values = c("TRUE" = "firebrick1", "FALSE" = "dodgerblue4"),
                     name = "Predicted state winner",
                     labels = c("Harris", "Trump")) +  # Custom color scale for the winner
  scale_shape_manual(values = c("Observed" = 4), 
                     name = "Actual results", 
                     labels = c("Observed Vote Share")) +  # Custom shape scale
  labs(
    title = "Predicted Democratic Two-Party Vote Share by State",
    x = "State",
    y = "Vote Share"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      angle = 90, 
      hjust = 1,
      color = ifelse(plotdata2024$state %in% swing_states, "purple", "black") # Color condition
    )
  )
```

Most of the actual results fall within the 95\% confidence interval. We note that states at the far end of the spectrum like Utah and Montana on the left and Massachusetts and Maryland on the right has a systematic regression towards the mean. This could mean that our model works best for tight races and the linear coefficients don't hold too well for strongholds. Call this **Problem A** which we will address later in a section. The model overestimates the support for the strongholds on both sides.

Note that within swing states, Harris overperformed our predictions across the board.

Here is the test RMSE for 2024
```{r}
sqrt(mean(
  (all_data|>filter(year==2024)|>mutate(r=(pv2p-preds)^2))$r
   ))
```

It can be interpreted that the actual results are within 1.5\% vote share difference with our predictions.


# National bias

I am also interested to investigate whether the error for each state is correlated in each election. If that's the case, that will make predictions harder since nice theoretical assumptions don't hold anymore, and the election results will be severely different from our forecasts if every state swings towards the same direction instead random swings whose effects cancel each other out.

```{r}
# find the bias
all_data$bias <- all_data$pv2p - all_data$preds
```

Here each dot represents a state and how much the model prediction is biased from the actual result. We see that on 2008 and 2016 the spread is centered around 0, indicating unbiasedness, while in 2012 the model had a national bias underestimating the incumbent (Obama) while in 2020 the model overestimated Trump and in 2024 it slightly overestimated Harris. Importantly, notice that the **bias of each state swings together**. This is important because it tells us that the bias is correlated across states in the same year.


```{r}
ggplot(all_data, aes(x = factor(year), y = bias)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width=0,height=0,alpha=0.3)+
  labs(title = "Deviation of model prediction from actual election result",
       x = "Years",
       y = "Bias favouring incumbents") +
  theme_minimal()
```

## Are certain states perculiar?

Let's see if there is any state that we consistently miss using our model.

```{r}
ggplot(all_data, aes(x = factor(state), y = bias)) +
  geom_violin(trim = FALSE) +
  labs(title = "Deviation of model prediction from actual election result",
       x = "States Predicted by the Model",
       y = "Bias favouring incumbents") +
  theme_minimal()+
  theme(axis.text.x = element_blank()) # Turns off x-axis labels
```

The plot shows that there are no states where we systematically overestimate or underestimate the incumbent over the years.

## GDP growth is bad for the incumbent?

Here are the coefficients for our final model

```{r}
model <- lm(formula = pv2p ~ POLL + Q2GDP + SIT + 
    LAG, data = all_data|>filter(year!=2024))
summary(model)
```

Notice that the coefficient for the GDP is negative. I suppose that this is because there is multi-colinearity among the predictors, which can result in the model to attribute some of the effects from the GDP to another closely associated predictor. Let's compute the Variance Inflation Factor (VIF) to check whether any values are greater than 10.

```{r}
# Load necessary library
library(car)

# Fit the linear model
model <- lm(pv2p ~ POLL + Q2GDP + SIT + LAG, data = subset(all_data, year != 2024))

# Compute VIF
vif_values <- vif(model)

# Display VIF values
print(vif_values)
```

Although GDP was not, it shows that POLL and LAG are closely associated. Let's look at the pairwise correlation.

```{r}
# Load necessary libraries
library(ggplot2)
library(reshape2) # For melting the correlation matrix into long format

# Select the predictors and filter for years != 2024
predictors <- subset(all_data, year != 2024)[, c("POLL", "Q2GDP", "SIT", "LAG")]

# Compute pairwise correlations
correlation_matrix <- cor(predictors)

# Convert correlation matrix to long format for ggplot
correlation_long <- melt(correlation_matrix)

# Plot the heatmap with numbers
ggplot(correlation_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 4) + # Add correlation numbers
  scale_fill_gradient2(low = "red", high = "blue", mid = "white", midpoint = 0,
                       name = "Correlation") +
  labs(
    title = "Correlation Heatmap of Predictors",
    x = "Predictor",
    y = "Predictor"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Since there is high correlation between LAG and POLL, we have to employ penalization. Call this **Problem B**.

## Addressing the problems

### Problem A

We first try to solve problem A by applying transformations. The linear model output doesn't limit to between 0 and 1, so we can attempt using a logistic regression instead.

```{r}
logitify <- function(x){log(x/(1-x))}
invlogitify <- function(x){exp(x)/(1+exp(x) )}
```


```{r}
# install.packages("betareg")
model <- lm(logitify(pv2p/100) ~ POLL + Q2GDP + SIT + LAG, 
                 data = subset(all_data, year != 2024))
newpreds <- invlogitify(predict(model,subset(all_data, year == 2024)))*100
rmse_log <- sqrt(mean( (newpreds - subset(all_data, year == 2024)$pv2p)^2 ))

model <- lm(pv2p ~ POLL + Q2GDP + SIT + LAG, 
                 data = subset(all_data, year != 2024))
newpreds <- predict(model,subset(all_data, year == 2024))
rmse_lm <- sqrt(mean( (newpreds - subset(all_data, year == 2024)$pv2p)^2 ))

cat("RMSE (linear): ", rmse_lm,"\n")
cat("RMSE (logit): ", rmse_log)
```

Logit transformation doesn't help. If we have more time, I would like to explore possible transformations so that the regression will output values between 0 to 1 and has better performance than our current model.

### Problem B

We will apply penalization via ridge regression as an experiment. The test data is 2024 and the train data are the 2008 to 2020 elections. We will scan through the penalty coefficient from $0.001$ to $100$.

```{r}
# Normalize numeric columns
# Apply normalization
scaled_all_data <- all_data
scaled_all_data$POLL <- scale(scaled_all_data$POLL)
scaled_all_data$Q2GDP <- scale(scaled_all_data$Q2GDP)
scaled_all_data$LAG <- scale(scaled_all_data$LAG)
scaled_all_data <- scaled_all_data |> select(POLL,Q2GDP, LAG,pv2p, SIT,year)

test_data <- subset(scaled_all_data,year==2024) |>select(-year)
train_data <- subset(scaled_all_data,year!=2024) |>select(-year)

lambdas <- 10^seq(-3, 2, length.out = 100)

# normalize
library(glmnet)
# Split data into predictors and response
y <- train_data$pv2p  # Replace with your response column name
X <- as.matrix(train_data[, -which(names(train_data) == "pv2p")])  # Exclude the response column
X_test <- as.matrix(test_data[, -which(names(test_data) == "pv2p")])

trainrmse <- c()
testrmse <- c()

for(lambda in lambdas){
  ridge_model <- glmnet(X, y, alpha=0,lambda = lambda)
  trainrmse <- c(trainrmse,sqrt(mean((predict(ridge_model, newx = X) - train_data$pv2p)^2)))
  testrmse <- c(testrmse,sqrt(mean((predict(ridge_model, newx = X_test) - test_data$pv2p)^2)))
}

# plot the train test loss over
```

```{r}
# Create a data frame for ggplot
rmse_data <- data.frame(
  Lambda = lambdas,
  TrainRMSE = trainrmse,
  TestRMSE = testrmse
)

# Reshape data for easier plotting
library(tidyr)
rmse_long <- rmse_data %>%
  pivot_longer(cols = c(TrainRMSE, TestRMSE), names_to = "Type", values_to = "RMSE")

# Plot with ggplot
ggplot(rmse_long, aes(x = log10(Lambda), y = RMSE, color = Type)) +
  geom_line(size = 1) +
  labs(
    title = "Train and Test RMSE vs Lambda",
    x = "log10(Lambda)",
    y = "RMSE",
    color = "Dataset"
  ) +
  theme_minimal()
```

Regularization works. The attentive reader might point out that the test RMSE is consistently lower than the train RMSE, but this is ok as a proof of concept and it might be the case that the 2024 election is more predictable than the others.

```{r}
rmse <- min(testrmse)
dex <- which.min(testrmse)
lambda <- lambdas[dex]

cat("Minimum 2024 RMSE is", rmse,'\n')
cat("Chosen lambda is", lambda)
```

The new model has a lower RMSE than our model.



The model predicts the following

```{r}
glm_model <- glmnet(X, y, alpha=0,lambda = lambda)
preds <- predict(glm_model,newx=X_test)
```

Here are the coefficients (the variables are standardized)

```{r}
coef(glm_model)
```

Unfortunately, the negative GDP situation is still there. Here are the predictions of the new model.

```{r}
plotdata2024 <- all_data|>filter(year==2024)
plotdata2024$state <- factor(plotdata2024$state, levels = plotdata2024$state)
plotdata2024$preds_glm <- preds

ggplot(plotdata2024, aes(x = state, y = preds)) +
  geom_point(aes(color = preds < 50), size = 1, shape = 16) +  # Dots for predicted vote share
  geom_point(aes(y = preds_glm, shape = "Ridge Regression", color = preds_glm < 50), size = 1) +  # Dots for ridge regression vote share
  geom_point(aes(y = pv2p, shape = "Observed", color = pv2p < 50), size = 2) +  # New marker for observed vote share
  geom_errorbar(aes(ymin = preds - allrmse, ymax = preds + allrmse), width = 0.2) +  # Error bars
  scale_color_manual(
    values = c("TRUE" = "firebrick1", "FALSE" = "dodgerblue4"),
    name = "Predicted State Winner",
    labels = c("Harris", "Trump")
  ) +  # Custom color scale for the winner
  scale_shape_manual(
    values = c("Observed" = 4, "Ridge Regression" = 2),
    name = "Result Types",
    labels = c("Observed Vote Share", "Ridge Regression")
  ) +  # Custom shape scale
  labs(
    title = "Predicted Democratic Two-Party Vote Share by State",
    x = "State",
    y = "Vote Share"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      angle = 90,
      hjust = 1,
      color = ifelse(plotdata2024$state %in% swing_states, "purple", "black")  # Color condition for swing states
    )
  )
```

Note that the ridge regression performed better than our model by making better predictions for the stronghold states, while the prediction for the swing states are relatively unchanged. In this case, we have also found a partial solution towards **Problem A**.

If we have more time, I would like to do a full-fledged model regularization, while incorporating a scheme where the training data and test data are partitioned by random years as batches. This will possibly yield a stronger model and likely solve the negative GDP problem.